# 数学知识

### 调和平均数

调和平均数（harmonic mean）又称倒数平均数，是总体各统计变量倒数的算术平均数的倒数。调和平均数有简单调和平均数和加权调和平均数两种。

简单调和平均数
$$
H_n= \frac{1}{\frac{1}{n}\sum\limits_{i=1}^n\frac{1}{x_i}}=\frac{n}{\sum\limits_{i=1}^n\frac{1}{x_i}}
$$

### 散度

散度（divergence）可用于表征空间各点矢量场发散的强弱程度

### KL散度(Kullback-Leibler Divergence)

KL散度，也叫相对熵。它衡量的是相同事件空间里的两个概率分布的差异情况，即衡量两个概率分布的相似性的一个度量指标。

信息熵定义如下：
$$
H=-\sum_{i=1}^Np(x_i)logp(x_i)
$$
p(xi)表示事件xi发生的概率，信息熵反映的是表示一个概率分布需要的平均信息量

KL散度定义为：
$$
D_{KL}(p||q)=\sum_{i=1}^Np(x_i)\cdot(log(p(x_i))-log(q(x_i))
$$
或者
$$
D_{KL}=\sum_{i=1}^Np(x_i)\cdot log\frac{p(x_i)}{q(x_i)}
$$
散度越小说明概率p和q之间越接近，那么估计的概率分布于真实的概率分布也就越接近

KL 散度可以帮助我们选择最优的参数，比如 p(x) 是我们需要估计的一个未知的分布，我们无法直接得知 p(x)的分布，不过我们可以建立一个分布 q(x∣θ)去估计 p(x)，为了确定参数 θ，虽然我们无法得知 p(x)的真实分布，但可以利用采样的方法，从 p(x)中采样 N个样本，构建如下的目标函数：
$$
D_{KL}=\sum_{i=1}^N{\{log(p(x_i))-log(q(x_i|\theta))\}}
$$
因为我们要预估的是参数 θ，上面的第一项 log ⁡p(xi) 与参数 θ无关，所以我们要优化的其实是 −log ⁡q(xi∣θ)，而这个就是我们熟悉的最大似然估计

### 最大似然估计

利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值

### Frobenius norm

对应元素的平方和再开方
$$
||A||_F=\sqrt{\sum_{i-1}^m\sum_{j=1}^n|a_{ij}|^2}
$$

### *汉明距离*

汉明距离是使用在数据传输差错控制编码里面的，汉明距离是一个概念，它表示两个（相同长度）字对应位不同的数量，我们以d（x,y）表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。