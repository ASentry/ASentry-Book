{"./":{"url":"./","title":"简介","keywords":"","body":"Introduction Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-09-29 09:53:32 "},"MoriningFlower/":{"url":"MoriningFlower/","title":"朝花夕拾","keywords":"","body":"朝花夕拾 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-09-29 14:50:42 "},"Essay/":{"url":"Essay/","title":"随笔","keywords":"","body":"随笔 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-02 15:24:22 "},"Essay/GoldenLine.html":{"url":"Essay/GoldenLine.html","title":"佳句","keywords":"","body":"佳句 我们的失败不是证明了我们的无能，而是证明了这座城市的颜色，是黑的。——《熔炉》 冬天之所以那么冷是为了告诉大家身边人的温暖有多重要。世界上最美丽最珍贵的，反而是听不见且看不清的，只能用心才能感受得到。——《熔炉》 每一桩罪案背后都有一公升的眼泪。——《狂探》 正如他们所说的那样,洞悉过人性以后才会明白,无论你是否做好了准备,生活都是一场随时可能被‘踢掉凳子’的绞刑。——《狂探》 有时候,你足够善良、足够热爱生活,却依旧无法阻挡这世上有着不可控制的人性之恶。——《狂探》 我认为,他们说得很对,究其根本,社会不公和缺乏教育才是滋生犯罪的土壤,才是最根本的原因!——《狂探》 我们都是行走在城市里孤独的人。在北京生活的人都知道，偌大的北京城，看起来人声鼎沸，可谁和谁又都没有关系，这是一个你哭得撕心裂肺，却没人停下来问你怎么了的地方。但这里也是所有温暖诉求的地方，你能在这里找到爱情，也能找到遗憾，城市永远不会变，它永远冰冷并且温暖地存在着，这就是北京。——《我有故事，你有酒吗？》 一生至少该有一次 ， 为了某个人而忘了自己 ， 不求有结果 ， 不求同行 ， 不求曾经拥有 ， 甚至不求你爱我 ， 只求在我最美的年华里 ， 遇到你 。       ——徐志摩 孤独这两个字拆开来看，有孩童，有瓜果，有小犬，有蚊蝇，足以撑起一个盛夏傍晚间的巷子口，人情味十足。稚儿擎瓜柳棚下，细犬逐蝶窄巷中，人间繁华多笑语，惟我空余两鬓风。——孩童水果猫狗飞蝇当然热闹，可都和你无关，这就叫孤独。——林语堂 这个世界并没有我们看上去那么的简单，人各有命上天注定，有人天生为王，有人落草为寇，脚下的路如果不是你自己的选择，那旅程的终点在哪儿也没人知道，你会走到哪会碰到谁都不一定。 ——镇魂街 读书给了我们一个到此一游的机会，让我们看遍了大观园，当我们以为我们余生都会在大观园生活的时候，生存却悄悄的告诉我们，我们很有可能，未来还是要回到来的地方。那一刻，我深深感受到了生存带来的无奈。 至亲离去的那一瞬间，通常不会使人感到悲伤，真正会让你感到悲痛的是打开冰箱的那半盒牛奶、那窗台上随风微曳的绿萝、那安静折叠在床上的绒被，还有那深夜里洗衣机传来的阵阵喧哗。 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-10 15:36:27 "},"CV/":{"url":"CV/","title":"CV笔记","keywords":"","body":"CV笔记 CV相关笔记 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-29 09:48:37 "},"CV/Convolutional-Neural-Networks.html":{"url":"CV/Convolutional-Neural-Networks.html","title":"卷积神经网络","keywords":"","body":"卷积神经网络 1.链式法则计算： \r y=f(x),z=g(y),\\frac{\\partial z}{\\partial x}=\\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x}\r 2.卷积核奇偶选择:奇数卷积核，奇数有利于原始数据对应。也可以选择偶数卷积核 3.上采样：输入图像经过卷积神经网络提取特征后，输出尺寸会变小，而将图像恢复到原来尺寸，实现小分辨率到大分辨率的映射操作叫上采样。上采样常见三种方法：双线性差值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling) 4.反卷积是一种特殊的正向卷积，先按照一定的比例通过补0来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。反卷积只能恢复尺寸，不能回复数值。 5.ReLU:将负值数据置0 6.池化就是降维 7.全卷积结构(FCN):没有全连接层。特点：1.输入图片大小无限制2.空间信息有丢失3.参数更少，表达能力更强 8.全局部卷积连接的缺陷： 预处理：大量对准，对对准要求高，原始信息可能丢失 卷积参数数量大，模型收敛难度大，需要大量数据 模型可扩展性3差，基本限于人脸计算 9.逆卷积：生成图片有更好的连贯性，有更好的空间表达能力 10.灰度化：在RGB模型中，如果R=G=B，则彩色表示一种灰度颜色，其中R=G=B的值叫做灰度值，因此灰度图像每个像素只需一个字节存放灰度值（又称强度值，亮度值），灰度范围为0-255 11.灰度化两种方法： 方法一 灰度化后的R=（处理前的R + 处理前的G +处理前的B）/ 3 灰度化后的G=（处理前的R + 处理前的G +处理前的B）/ 3 灰度化后的B=（处理前的R + 处理前的G +处理前的B）/ 3 方法二 灰度化后的R = 处理前的R 0.3+ 处理前的G 0.59 +处理前的B * 0.11 灰度化后的G = 处理前的R 0.3+ 处理前的G 0.59 +处理前的B * 0.11 灰度化后的B = 处理前的R 0.3+ 处理前的G 0.59 +处理前的B * 0.11 12.贝叶斯公式：P(A|B)=P(B|A)*P(A)/P(B) 13.IoU=A∩B/A∪B 14.如果导数为负，则参数向更大的方向走，反之，向更小的方向走。学习率控制变化速度 15.越复杂的model不一定会获得更好的结果，可能发生过拟合(overfit) 16.传统梯度下降： 17.动量梯度下降： 18.Dropout:每次更新参数时都要随机丢掉一部分神经元，对新的神经网络进行训练。训练时准确率下降，但测试时会上升。测试时不dropout,但权重要乘以(1-p%)(p%是训练时设置的dropout rate) 19.RNN:将之前的输出存贮起来，下次使用后更新 20.Elman NetWork&Jordan Network 21.Bidirectional RNN:看的范围比较广 22.LSTM: 23.RNN换为LSTM的原因：LSTM可以解决梯度消失的问题。 24.CTC:用NULL代替重复符号 25.Self-Attention： Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-28 16:15:39 "},"CV/Image-Preprocessing-Noise.html":{"url":"CV/Image-Preprocessing-Noise.html","title":"图像预处理-噪声","keywords":"","body":"图像预处理-噪声 噪声主要分为以下几类： 椒盐噪声 加性噪声 乘性噪声 高斯噪声 图像去噪的方法有很多种，其中均值滤波、中值滤波等比较基础且成熟，还有一些基于数学中偏微分方程的去噪方法，此外，还有基于频域的小波去噪方法。均值滤波、中值滤波这些基础的去噪算法以其快速、稳定等特性，在项目中非常受欢迎，在很多成熟的软件或者工具包中也集成了这些算法。 代码： import cv2 import numpy as np import skimage from skimage.util.dtype import convert # 读取图像 img = cv2.imread(\"/Opencvproject/img2007/2007_001458.jpg\") # cv2.imshow(\"origin_img\", img) # cv2.waitKey(0) # cv2.destroyAllWindows() # #添加噪声 #1.利用第三方工具添加噪声 noise_img = skimage.util.random_noise(img,mode=\"gaussian\") # cv2.imshow(\"origin1_img\", noise_img) # cv2.waitKey() # #2.自己生成噪声 # def add_noise(img): # img = np.multiply(img, 1. / 255, dtype=np.float64) # mean, var = 0, 0.01 # noise = np.random.normal(mean, var ** 0.5, img.shape) # img = convert(img, np.floating) # out = img + noise # return out # # # noise_img = add_noise(img) # gray_img = cv2.cvtColor(noise_img, cv2.COLOR_BGR2GRAY) # cv2.imwrite(\"D:/Opencvproject/img2007/handnoise.jpg\", noise_img) # 3.图像去噪 # 方法1：用第三方工具去噪 # denoise = cv2.medianBlur(img, ksize=3) # denoise = cv2.fastNlMeansDenoising(img, ksize=3) # denoise = cv2.GaussianBlur(img, ksize=3) # def compute_pixel_value(img, i, j, ksize, channel): # h_begin = max(0, i - ksize // 2) # h_end = min(img.shape[0], i + ksize // 2) # w_begin = max(0, j - ksize // 2) # w_end = min(img.shape[1], j + ksize // 2) # return np.median(img[h_begin:h_end, w_begin:w_end, channel]) # # def denoise(img, ksize): # output = np.zeros(img.shape) # for i in range(img.shape[0]): # for j in range(img.shape[1]): # output[i, j, 0] = compute_pixel_value(img, i, j, ksize, 0) # output[i, j, 1] = compute_pixel_value(img, i, j, ksize, 1) # output[i, j, 2] = compute_pixel_value(img, i, j, ksize, 2) # return output # # output = denoise(noise_img, 3) # cv2.imshow(\"denoise_img\", output) # cv2.waitKey(0) # cv2.destroyAllWindows() Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 10:11:35 "},"CV/Image-Preprocessing-Enhance.html":{"url":"CV/Image-Preprocessing-Enhance.html","title":"图像预处理-增强","keywords":"","body":"图像预处理-增强 图像增强可分为两类： 频域法 空间域法 频域法，顾名思义，频域法就是把图像从空域利用傅立叶、小波变换等算法把图像从空间域转化成频域，也就是把图像矩阵转化成二维信号，进而使用高通滤波或低通滤波器对信号进行过滤。采用低通滤波器（即只让低频信号通过）法，可去掉图中的噪声；采用高通滤波法，则可增强边缘等高频信号，使模糊的图片变得清晰。 其次，介绍一下空域方法，空域方法用的比较多，空域方法主要包括以下几种常用的算法： 直方图均衡化 滤波 滤波 基于滤波的算法主要包括以下几种： 均值滤波 中值滤波 高斯滤波 代码： import cv2 # 读取图像并转化为灰度图 img = cv2.imread(\"/Opencvproject/img2007/2007_000793.jpg\") gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY) cv2.imshow(\"gray\", gray) cv2.waitKey(0) cv2.destroyAllWindows() # 显示灰度直方图 # opencv calcHist函数传入5个参数： # images：图像 # channels：通道 # mask：图像掩码，可以填写None # hisSize：灰度数目 # ranges：回复分布区间 def histogram(gray): hist = cv2.calcHist([gray], [0], None, [256], [0.0, 255.0]) plt.plot(range(len(hist)), hist) plt.show() histogram(gray) # 直方图均衡化 dst = cv2.equalizeHist(gray) histogram(dst) Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 10:13:35 "},"CV/Image-Preprocessing-Segmentation.html":{"url":"CV/Image-Preprocessing-Segmentation.html","title":"图像预处理-分割","keywords":"","body":"图像预处理-分割 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 09:44:20 "},"CV/Image-Preprocessing-Augmentation.html":{"url":"CV/Image-Preprocessing-Augmentation.html","title":"图像预处理-增广","keywords":"","body":"图像预处理-增广 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 09:44:20 "},"CV/Attention.html":{"url":"CV/Attention.html","title":"Attention机制","keywords":"","body":"Attention机制 Encoder-Decoder框架 Encoder-Decoder框架是将输入的句子进行非线性变换变成中间语义C,再由解码器利用中间语义和历史信息生成目标语句 Encoder-Decoder没有注意力模块所以每次参与解码的中间语义信息均相同 y_1 = f(C)\\\\ y_2 = f(C,y_1)\\\\ y_3 = f(C,y_1,y_2)\\\\ ... Soft Attention Attention机制则根据其影响程度给每个单词一个概率，即该单词的注意力大小，如 Tom chase Jerry (Tom,0.3)(Chase,0.2)(Jerry,0.5) 解码器对不同的输入单词采用不同的中间语义信息解码 y_1 = f(C1)\\\\ y_2 = f(C2,y_1)\\\\ y_3 = f(C3,y_1,y_2)\\\\ ... Self Attention 首先计算Q和K之间的点乘，其意义为句中每每两词的相关度。为防止其过大会除以一个尺度标量 \\sqrt{d_k}，d_k为为一个query和key向量的维度 再利用softmax归一化为概率分布，再乘以矩阵V就得到权重求和表示，公式 Attention(Q,K,V) = softmax(\\frac{QK^t}{\\sqrt{d_k}})V Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-28 22:02:59 "},"CV/CTC-RNN-T.html":{"url":"CV/CTC-RNN-T.html","title":"CTC,RNN-T","keywords":"","body":"CTC,RNN-T CTC CTC全称Connectionist Temporal Classification,主要是为了解决利用RNN训练时需要目标label与输入的每一帧需要alignment的问题，即我们需要知道哪几帧输入对应输出的哪个字符并且知道如何分割不同输出字符对应的输入帧的边界，而且有的时候这种边界较为模糊，这种需要逐帧对应的标记的数据相较于只是需要简单的文字输出的人力要求要高很多。 为了解决alignment的问题，CTC定义了blank symbol来做填充，假定填充符号为—，如果我们输入的音频有八帧，而对应label为cat只有三个字符则，CTC可以做如下填充 输入语音为\\vec{x}=x_1x_2\\cdot\\cdot\\cdot x_n对应label为\\vec{y}=y_1y_2\\cdot\\cdot\\cdot y_n\\\\ cc-aa-t-\\\\ c-aa-tt-\\\\ c-aaa-t-\\\\ ...\\\\ \\sum_{\\hat{y}\\in B(\\vec{y},\\vec{x})}P(\\hat{y},\\vec{x})=\\sum_{\\hat{y}\\in B(\\vec{y},\\vec{x})}\\Pi_{t=1}^T P(\\hat{y_t},\\vec{x}) 该模型可用下图表示： RNN-T RNN-T全称是Recurrent Neural Network Transducer，是在CTC的基础上改进的。CTC的缺点是它没有考虑输出之间的dependency，即之后的帧和之前的帧没有任何关联，而RNN-T则在CTC模型的Encoder基础上，又加入了将之前的输出作为输入的一个RNN，称为Prediction Network，再将其输出的隐藏向量与encoder得到的放到一个joint network中，得到输出logit再将其传到softmax layer得到对应的class的概率。 结构如下： Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-29 11:00:41 "},"CV/Likelihood-Function.html":{"url":"CV/Likelihood-Function.html","title":"似然函数","keywords":"","body":"似然函数 似然函数(likelihood function) Likelihood is used to describe the plausibility of a value for the parameter, given some data. —from wikipedia 其数学形式为： 假设X是观测结果序列，它的概率分布fx依赖于参数θ，则似然函数表示为 L(\\theta\\mid x) = f_\\theta(x)=P_\\theta(X=x) 离散型概率分布(Discrete probability distributions) 假设X是离散随机变量,其概率质量函数p依赖于参数θ,则有 L(\\theta\\mid x) = p_\\theta(x)=P_\\theta(X=x) 连续型概率分布(Continuous probability distributions) 假设X是连续概率分布的随机变量,其密度函数f依赖于参数θ ,则有 L(\\theta\\mid x) = f_\\theta(x) 最大似然估计(Maximum Likelihood Estimation,MLE) 假设每个观测结果x是独立同分布的，通过似然函数L(θ|x)，求使观测结果X发生的概率最大的参数θ，即argmaxf(X;θ) 对数似然估计(log likelihood) 由于对数函数具有单调递增的特点，对数函数和似然函数具有同一个最大值点。取对数是为了方便计算极大似然估计，MLE中直接求导比较困难，通常先取对数再求导，找到极值点。 负对数似然估计(negative log-likelihood) -logP(y\\mid x) Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-29 22:28:59 "},"CV/ResNet.html":{"url":"CV/ResNet.html","title":"ResNet","keywords":"","body":"ResNet ResNet结构图 1.为什么会有ResNet？ 神经网络堆叠不一定会使得效果更好，模型会出现退化现象。ResNet为了解决模型退化问题而提出，使得网络更深成为可能。 ResNet更倾向于均摊任务而不是让他一部分结点什么都不做 -----来自知乎 2.Pytorch官方ResNet代码 import torch from torch import Tensor import torch.nn as nn from torch.utils.model_zoo import load_url as load_state_dict_from_url from typing import Type, Any, Callable, Union, List, Optional __all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d','resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2'] model_urls = { 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth', 'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth', 'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth', } # def()->xxx xxx表明返回值类型 # dilation 卷积像素的间隔，0无间隔，1每个像素之间间隔一个感受域增大 # groups 分组卷积，groups必须能整除in_channels和out_channels def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d: \"\"\"3x3卷积层\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False,dilation=dilation) def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d: \"\"\"1x1卷积层\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) # 自定义层、自定义块、自定义模型，都是通过继承Module类完成的 # 我们在定义自已的网络的时候，需要继承nn.Module类，并重新实现构造函数__init__ #构造函数和forward这两个方法。但有一些注意技巧： # （1）一般把网络中具有可学习参数的层（如全连接层、卷积层等）放在构造函数__init__()中， #当然我也可以吧不具有参数的层也放在里面； # （2）一般把不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)可放在构造函数中， #也可不放在构造函数中，如果不放在构造函数__init__里面，则在forward方法里面可以使用nn.functional来代替 # （3）forward方法是必须要重写的，它是实现模型的功能，实现各个层之间的连接关系的核心。 # nn.Relu(inplace=True)设置为True会改变输入数据的值，节约内存时间 class BasicBlock(nn.Module): expansion: int = 1 def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None: # super调用父类的一个方法 super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d # raise手动设置异常 if groups != 1 or base_width != 64: raise ValueError('BasicBlock only supports groups=1 and base_width=64') if dilation > 1: raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\") self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x: Tensor) -> Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out class Bottleneck(nn.Module): expansion: int = 4 def __init__( self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None ) -> None: super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x: Tensor) -> Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out class ResNet(nn.Module): def __init__(self, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None: super(ResNet, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d self._norm_layer = norm_layer self.inplanes = 64 self.dilation = 1 if replace_stride_with_dilation is None: replace_stride_with_dilation = [False, False, False] if len(replace_stride_with_dilation) != 3: raise ValueError(\"replace_stride_with_dilation should be None \" \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)) self.groups = groups self.base_width = width_per_group self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = norm_layer(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): # isinstance()判断变量是否为指类型 if isinstance(m, nn.Conv2d): # 权重初始化 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): # 固定值初始化 nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int, stride: int = 1, dilate: bool = False) -> nn.Sequential: norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if dilate: self.dilation *= stride stride = 1 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion) ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups , self.base_width, previous_dilation,norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append( block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def _forward_impl(self, x: Tensor) -> Tensor: x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x def forward(self, x: Tensor) -> Tensor: return self._forward_impl(x) def _resnet(arch: str, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], pretrained: bool, progress: bool, **kwargs: Any) -> ResNet: model = ResNet(block, layers, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return model def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs) def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs) def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['groups'] = 32 kwargs['width_per_group'] = 4 return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['groups'] = 32 kwargs['width_per_group'] = 8 return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-03 15:38:54 "}}