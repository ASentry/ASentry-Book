{"./":{"url":"./","title":"简介","keywords":"","body":"Introduction Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-09-29 09:53:32 "},"MoriningFlower/":{"url":"MoriningFlower/","title":"朝花夕拾","keywords":"","body":"朝花夕拾 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-09-29 14:50:42 "},"Essay/":{"url":"Essay/","title":"随笔","keywords":"","body":"随笔 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-02 15:24:22 "},"Essay/GoldenLine.html":{"url":"Essay/GoldenLine.html","title":"佳句","keywords":"","body":"佳句 我们的失败不是证明了我们的无能，而是证明了这座城市的颜色，是黑的。——《熔炉》 冬天之所以那么冷是为了告诉大家身边人的温暖有多重要。世界上最美丽最珍贵的，反而是听不见且看不清的，只能用心才能感受得到。——《熔炉》 每一桩罪案背后都有一公升的眼泪。——《狂探》 正如他们所说的那样,洞悉过人性以后才会明白,无论你是否做好了准备,生活都是一场随时可能被‘踢掉凳子’的绞刑。——《狂探》 有时候,你足够善良、足够热爱生活,却依旧无法阻挡这世上有着不可控制的人性之恶。——《狂探》 我认为,他们说得很对,究其根本,社会不公和缺乏教育才是滋生犯罪的土壤,才是最根本的原因!——《狂探》 我们都是行走在城市里孤独的人。在北京生活的人都知道，偌大的北京城，看起来人声鼎沸，可谁和谁又都没有关系，这是一个你哭得撕心裂肺，却没人停下来问你怎么了的地方。但这里也是所有温暖诉求的地方，你能在这里找到爱情，也能找到遗憾，城市永远不会变，它永远冰冷并且温暖地存在着，这就是北京。——《我有故事，你有酒吗？》 一生至少该有一次 ， 为了某个人而忘了自己 ， 不求有结果 ， 不求同行 ， 不求曾经拥有 ， 甚至不求你爱我 ， 只求在我最美的年华里 ， 遇到你 。       ——徐志摩 孤独这两个字拆开来看，有孩童，有瓜果，有小犬，有蚊蝇，足以撑起一个盛夏傍晚间的巷子口，人情味十足。稚儿擎瓜柳棚下，细犬逐蝶窄巷中，人间繁华多笑语，惟我空余两鬓风。——孩童水果猫狗飞蝇当然热闹，可都和你无关，这就叫孤独。——林语堂 这个世界并没有我们看上去那么的简单，人各有命上天注定，有人天生为王，有人落草为寇，脚下的路如果不是你自己的选择，那旅程的终点在哪儿也没人知道，你会走到哪会碰到谁都不一定。 ——镇魂街 读书给了我们一个到此一游的机会，让我们看遍了大观园，当我们以为我们余生都会在大观园生活的时候，生存却悄悄的告诉我们，我们很有可能，未来还是要回到来的地方。那一刻，我深深感受到了生存带来的无奈。 至亲离去的那一瞬间，通常不会使人感到悲伤，真正会让你感到悲痛的是打开冰箱的那半盒牛奶、那窗台上随风微曳的绿萝、那安静折叠在床上的绒被，还有那深夜里洗衣机传来的阵阵喧哗。 一朝漂流过，百里不回头 ### 我不知道我丢了什么，但确实丢了 ### 说谎并不可怕，可怕的是最终我们把它圆了回来 ### 聪明不是智慧，或许我的智慧在他人眼中也只是愚昧吧 ### 见过太多，听过太多，经历太多，却总想要装作单纯，过早成熟或许是一种悲哀 ### 坐在海边，吹着海风，脑子里一片空白，只觉得身心都轻松了许多。如今只记得那天在海边坐了好久，从那以后再没有那种体会 ### Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-12-02 22:21:31 "},"CV/":{"url":"CV/","title":"CV笔记","keywords":"","body":"CV笔记 CV相关笔记 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-29 09:48:37 "},"CV/Convolutional-Neural-Networks.html":{"url":"CV/Convolutional-Neural-Networks.html","title":"卷积神经网络","keywords":"","body":"卷积神经网络 1.链式法则计算： \r y=f(x),z=g(y),\\frac{\\partial z}{\\partial x}=\\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x}\r 2.卷积核奇偶选择:奇数卷积核，奇数有利于原始数据对应。也可以选择偶数卷积核 3.上采样：输入图像经过卷积神经网络提取特征后，输出尺寸会变小，而将图像恢复到原来尺寸，实现小分辨率到大分辨率的映射操作叫上采样。上采样常见三种方法：双线性差值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling) 4.反卷积是一种特殊的正向卷积，先按照一定的比例通过补0来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。反卷积只能恢复尺寸，不能回复数值。 5.ReLU:将负值数据置0 6.池化就是降维 7.全卷积结构(FCN):没有全连接层。特点：1.输入图片大小无限制2.空间信息有丢失3.参数更少，表达能力更强 8.全局部卷积连接的缺陷： 预处理：大量对准，对对准要求高，原始信息可能丢失 卷积参数数量大，模型收敛难度大，需要大量数据 模型可扩展性3差，基本限于人脸计算 9.逆卷积：生成图片有更好的连贯性，有更好的空间表达能力 10.灰度化：在RGB模型中，如果R=G=B，则彩色表示一种灰度颜色，其中R=G=B的值叫做灰度值，因此灰度图像每个像素只需一个字节存放灰度值（又称强度值，亮度值），灰度范围为0-255 11.灰度化两种方法： 方法一 灰度化后的R=（处理前的R + 处理前的G +处理前的B）/ 3 灰度化后的G=（处理前的R + 处理前的G +处理前的B）/ 3 灰度化后的B=（处理前的R + 处理前的G +处理前的B）/ 3 方法二 灰度化后的R = 处理前的R 0.3+ 处理前的G 0.59 +处理前的B * 0.11 灰度化后的G = 处理前的R 0.3+ 处理前的G 0.59 +处理前的B * 0.11 灰度化后的B = 处理前的R 0.3+ 处理前的G 0.59 +处理前的B * 0.11 12.贝叶斯公式：P(A|B)=P(B|A)*P(A)/P(B) 13.IoU=A∩B/A∪B 14.如果导数为负，则参数向更大的方向走，反之，向更小的方向走。学习率控制变化速度 15.越复杂的model不一定会获得更好的结果，可能发生过拟合(overfit) 16.传统梯度下降： 17.动量梯度下降： 18.Dropout:每次更新参数时都要随机丢掉一部分神经元，对新的神经网络进行训练。训练时准确率下降，但测试时会上升。测试时不dropout,但权重要乘以(1-p%)(p%是训练时设置的dropout rate) 19.RNN:将之前的输出存贮起来，下次使用后更新 20.Elman NetWork&Jordan Network 21.Bidirectional RNN:看的范围比较广 22.LSTM: 23.RNN换为LSTM的原因：LSTM可以解决梯度消失的问题。 24.CTC:用NULL代替重复符号 25.Self-Attention： 26.激活函数 激活函数分为：饱和激活函数和非饱和激活函数 1）饱和激活函数：能解决梯度消失问题，能加快收敛速度，如sigmod,tanh 2）非饱和激活函数： ReLu ELUs:指数线性单元，它试图将激活函数的平均值接近零，从而加快学习速度。还能通过正值标识来避免梯度消失 Leaky ReLUs:给所有负值赋予一个非零斜率 PReLU:Leaky ReLUs的变体。负值部分斜率是根据数据来定的，而非预先定义。 RReLU:Leaky ReLU的变体，训练中负值斜率随机，测试时固定 下图是ReLU,Leaky ReLU,PReLU,RReLU的比较： Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-20 09:57:09 "},"CV/Image-Preprocessing-Noise.html":{"url":"CV/Image-Preprocessing-Noise.html","title":"图像预处理-噪声","keywords":"","body":"图像预处理-噪声 噪声主要分为以下几类： 椒盐噪声 加性噪声 乘性噪声 高斯噪声 图像去噪的方法有很多种，其中均值滤波、中值滤波等比较基础且成熟，还有一些基于数学中偏微分方程的去噪方法，此外，还有基于频域的小波去噪方法。均值滤波、中值滤波这些基础的去噪算法以其快速、稳定等特性，在项目中非常受欢迎，在很多成熟的软件或者工具包中也集成了这些算法。 代码： import cv2 import numpy as np import skimage from skimage.util.dtype import convert # 读取图像 img = cv2.imread(\"/Opencvproject/img2007/2007_001458.jpg\") # cv2.imshow(\"origin_img\", img) # cv2.waitKey(0) # cv2.destroyAllWindows() # #添加噪声 #1.利用第三方工具添加噪声 noise_img = skimage.util.random_noise(img,mode=\"gaussian\") # cv2.imshow(\"origin1_img\", noise_img) # cv2.waitKey() # #2.自己生成噪声 # def add_noise(img): # img = np.multiply(img, 1. / 255, dtype=np.float64) # mean, var = 0, 0.01 # noise = np.random.normal(mean, var ** 0.5, img.shape) # img = convert(img, np.floating) # out = img + noise # return out # # # noise_img = add_noise(img) # gray_img = cv2.cvtColor(noise_img, cv2.COLOR_BGR2GRAY) # cv2.imwrite(\"D:/Opencvproject/img2007/handnoise.jpg\", noise_img) # 3.图像去噪 # 方法1：用第三方工具去噪 # denoise = cv2.medianBlur(img, ksize=3) # denoise = cv2.fastNlMeansDenoising(img, ksize=3) # denoise = cv2.GaussianBlur(img, ksize=3) # def compute_pixel_value(img, i, j, ksize, channel): # h_begin = max(0, i - ksize // 2) # h_end = min(img.shape[0], i + ksize // 2) # w_begin = max(0, j - ksize // 2) # w_end = min(img.shape[1], j + ksize // 2) # return np.median(img[h_begin:h_end, w_begin:w_end, channel]) # # def denoise(img, ksize): # output = np.zeros(img.shape) # for i in range(img.shape[0]): # for j in range(img.shape[1]): # output[i, j, 0] = compute_pixel_value(img, i, j, ksize, 0) # output[i, j, 1] = compute_pixel_value(img, i, j, ksize, 1) # output[i, j, 2] = compute_pixel_value(img, i, j, ksize, 2) # return output # # output = denoise(noise_img, 3) # cv2.imshow(\"denoise_img\", output) # cv2.waitKey(0) # cv2.destroyAllWindows() Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 10:11:35 "},"CV/Image-Preprocessing-Enhance.html":{"url":"CV/Image-Preprocessing-Enhance.html","title":"图像预处理-增强","keywords":"","body":"图像预处理-增强 图像增强可分为两类： 频域法 空间域法 频域法，顾名思义，频域法就是把图像从空域利用傅立叶、小波变换等算法把图像从空间域转化成频域，也就是把图像矩阵转化成二维信号，进而使用高通滤波或低通滤波器对信号进行过滤。采用低通滤波器（即只让低频信号通过）法，可去掉图中的噪声；采用高通滤波法，则可增强边缘等高频信号，使模糊的图片变得清晰。 其次，介绍一下空域方法，空域方法用的比较多，空域方法主要包括以下几种常用的算法： 直方图均衡化 滤波 滤波 基于滤波的算法主要包括以下几种： 均值滤波 中值滤波 高斯滤波 代码： import cv2 # 读取图像并转化为灰度图 img = cv2.imread(\"/Opencvproject/img2007/2007_000793.jpg\") gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY) cv2.imshow(\"gray\", gray) cv2.waitKey(0) cv2.destroyAllWindows() # 显示灰度直方图 # opencv calcHist函数传入5个参数： # images：图像 # channels：通道 # mask：图像掩码，可以填写None # hisSize：灰度数目 # ranges：回复分布区间 def histogram(gray): hist = cv2.calcHist([gray], [0], None, [256], [0.0, 255.0]) plt.plot(range(len(hist)), hist) plt.show() histogram(gray) # 直方图均衡化 dst = cv2.equalizeHist(gray) histogram(dst) Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 10:13:35 "},"CV/Image-Preprocessing-Segmentation.html":{"url":"CV/Image-Preprocessing-Segmentation.html","title":"图像预处理-分割","keywords":"","body":"图像预处理-分割 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 09:44:20 "},"CV/Image-Preprocessing-Augmentation.html":{"url":"CV/Image-Preprocessing-Augmentation.html","title":"图像预处理-增广","keywords":"","body":"图像预处理-增广 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-23 09:44:20 "},"CV/Attention.html":{"url":"CV/Attention.html","title":"Attention机制","keywords":"","body":"Attention机制 Encoder-Decoder框架 Encoder-Decoder框架是将输入的句子进行非线性变换变成中间语义C,再由解码器利用中间语义和历史信息生成目标语句 Encoder-Decoder没有注意力模块所以每次参与解码的中间语义信息均相同 y_1 = f(C)\\\\ y_2 = f(C,y_1)\\\\ y_3 = f(C,y_1,y_2)\\\\ ... Soft Attention Attention机制则根据其影响程度给每个单词一个概率，即该单词的注意力大小，如 Tom chase Jerry (Tom,0.3)(Chase,0.2)(Jerry,0.5) 解码器对不同的输入单词采用不同的中间语义信息解码 y_1 = f(C1)\\\\ y_2 = f(C2,y_1)\\\\ y_3 = f(C3,y_1,y_2)\\\\ ... Self Attention 首先计算Q和K之间的点乘，其意义为句中每每两词的相关度。为防止其过大会除以一个尺度标量 \\sqrt{d_k}，d_k为为一个query和key向量的维度 再利用softmax归一化为概率分布，再乘以矩阵V就得到权重求和表示，公式 Attention(Q,K,V) = softmax(\\frac{QK^t}{\\sqrt{d_k}})V Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-28 22:02:59 "},"CV/CTC-RNN-T.html":{"url":"CV/CTC-RNN-T.html","title":"CTC,RNN-T","keywords":"","body":"CTC,RNN-T CTC CTC全称Connectionist Temporal Classification,主要是为了解决利用RNN训练时需要目标label与输入的每一帧需要alignment的问题，即我们需要知道哪几帧输入对应输出的哪个字符并且知道如何分割不同输出字符对应的输入帧的边界，而且有的时候这种边界较为模糊，这种需要逐帧对应的标记的数据相较于只是需要简单的文字输出的人力要求要高很多。 为了解决alignment的问题，CTC定义了blank symbol来做填充，假定填充符号为—，如果我们输入的音频有八帧，而对应label为cat只有三个字符则，CTC可以做如下填充 输入语音为\\vec{x}=x_1x_2\\cdot\\cdot\\cdot x_n对应label为\\vec{y}=y_1y_2\\cdot\\cdot\\cdot y_n\\\\ cc-aa-t-\\\\ c-aa-tt-\\\\ c-aaa-t-\\\\ ...\\\\ \\sum_{\\hat{y}\\in B(\\vec{y},\\vec{x})}P(\\hat{y},\\vec{x})=\\sum_{\\hat{y}\\in B(\\vec{y},\\vec{x})}\\Pi_{t=1}^T P(\\hat{y_t},\\vec{x}) 该模型可用下图表示： RNN-T RNN-T全称是Recurrent Neural Network Transducer，是在CTC的基础上改进的。CTC的缺点是它没有考虑输出之间的dependency，即之后的帧和之前的帧没有任何关联，而RNN-T则在CTC模型的Encoder基础上，又加入了将之前的输出作为输入的一个RNN，称为Prediction Network，再将其输出的隐藏向量与encoder得到的放到一个joint network中，得到输出logit再将其传到softmax layer得到对应的class的概率。 结构如下： Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-29 11:00:41 "},"CV/Likelihood-Function.html":{"url":"CV/Likelihood-Function.html","title":"似然函数","keywords":"","body":"似然函数 似然函数(likelihood function) Likelihood is used to describe the plausibility of a value for the parameter, given some data. —from wikipedia 其数学形式为： 假设X是观测结果序列，它的概率分布fx依赖于参数θ，则似然函数表示为 L(\\theta\\mid x) = f_\\theta(x)=P_\\theta(X=x) 离散型概率分布(Discrete probability distributions) 假设X是离散随机变量,其概率质量函数p依赖于参数θ,则有 L(\\theta\\mid x) = p_\\theta(x)=P_\\theta(X=x) 连续型概率分布(Continuous probability distributions) 假设X是连续概率分布的随机变量,其密度函数f依赖于参数θ ,则有 L(\\theta\\mid x) = f_\\theta(x) 最大似然估计(Maximum Likelihood Estimation,MLE) 假设每个观测结果x是独立同分布的，通过似然函数L(θ|x)，求使观测结果X发生的概率最大的参数θ，即argmaxf(X;θ) 对数似然估计(log likelihood) 由于对数函数具有单调递增的特点，对数函数和似然函数具有同一个最大值点。取对数是为了方便计算极大似然估计，MLE中直接求导比较困难，通常先取对数再求导，找到极值点。 负对数似然估计(negative log-likelihood) -logP(y\\mid x) Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-10-29 22:28:59 "},"CV/ResNet.html":{"url":"CV/ResNet.html","title":"ResNet","keywords":"","body":"ResNet ResNet结构图 1.为什么会有ResNet？ 神经网络堆叠不一定会使得效果更好，模型会出现退化现象。ResNet为了解决模型退化问题而提出，使得网络更深成为可能。 ResNet更倾向于均摊任务而不是让他一部分结点什么都不做 -----来自知乎 2.Pytorch官方ResNet代码 import torch from torch import Tensor import torch.nn as nn from torch.utils.model_zoo import load_url as load_state_dict_from_url from typing import Type, Any, Callable, Union, List, Optional __all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d','resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2'] model_urls = { 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth', 'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth', 'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth', } # def()->xxx xxx表明返回值类型 # dilation 卷积像素的间隔，0无间隔，1每个像素之间间隔一个感受域增大 # groups 分组卷积，groups必须能整除in_channels和out_channels def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d: \"\"\"3x3卷积层\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False,dilation=dilation) def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d: \"\"\"1x1卷积层\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) # 自定义层、自定义块、自定义模型，都是通过继承Module类完成的 # 我们在定义自已的网络的时候，需要继承nn.Module类，并重新实现构造函数__init__ #构造函数和forward这两个方法。但有一些注意技巧： # （1）一般把网络中具有可学习参数的层（如全连接层、卷积层等）放在构造函数__init__()中， #当然我也可以吧不具有参数的层也放在里面； # （2）一般把不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)可放在构造函数中， #也可不放在构造函数中，如果不放在构造函数__init__里面，则在forward方法里面可以使用nn.functional来代替 # （3）forward方法是必须要重写的，它是实现模型的功能，实现各个层之间的连接关系的核心。 # nn.Relu(inplace=True)设置为True会改变输入数据的值，节约内存时间 class BasicBlock(nn.Module): expansion: int = 1 def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None: # super调用父类的一个方法 super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d # raise手动设置异常 if groups != 1 or base_width != 64: raise ValueError('BasicBlock only supports groups=1 and base_width=64') if dilation > 1: raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\") self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x: Tensor) -> Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out class Bottleneck(nn.Module): expansion: int = 4 def __init__( self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None ) -> None: super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x: Tensor) -> Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out class ResNet(nn.Module): def __init__(self, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None: super(ResNet, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d self._norm_layer = norm_layer self.inplanes = 64 self.dilation = 1 if replace_stride_with_dilation is None: replace_stride_with_dilation = [False, False, False] if len(replace_stride_with_dilation) != 3: raise ValueError(\"replace_stride_with_dilation should be None \" \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)) self.groups = groups self.base_width = width_per_group self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = norm_layer(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): # isinstance()判断变量是否为指类型 if isinstance(m, nn.Conv2d): # 权重初始化 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): # 固定值初始化 nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int, stride: int = 1, dilate: bool = False) -> nn.Sequential: norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if dilate: self.dilation *= stride stride = 1 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion) ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups , self.base_width, previous_dilation,norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append( block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def _forward_impl(self, x: Tensor) -> Tensor: x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x def forward(self, x: Tensor) -> Tensor: return self._forward_impl(x) def _resnet(arch: str, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], pretrained: bool, progress: bool, **kwargs: Any) -> ResNet: model = ResNet(block, layers, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return model def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs) def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs) def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['groups'] = 32 kwargs['width_per_group'] = 4 return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['groups'] = 32 kwargs['width_per_group'] = 8 return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet: kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-03 15:38:54 "},"CV/Markov.html":{"url":"CV/Markov.html","title":"马尔可夫链","keywords":"","body":"马尔可夫链 马尔可夫模型就是，无论一件事情初始状态是什么，也不论中间过程有什么样的一次性干预，事情终究会演化成一个平衡态。其中每个状态所占的比例是不变的。----来自知乎用户漂泊日本 马尔可夫性质 马尔可夫性质（英语：Markov property）是概率论中的一个概念，因为俄国数学家安德雷·马尔可夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。具有马尔可夫性质的过程通常称之为马尔可夫过程 定义 数学上，如果X(t),t>0为一个随机过程，则马尔可夫性质就是指 Pr[X(t+h)=y|X(s)=x(s),s\\leq t]=Pr[X(t+h)=y|X(t)=x(t)],\\forall h>0 马尔可夫过程通常称其为（时间）齐次，如果满足 Pr[X(t+h)=y|X(t)=x(t)]=Pr[X(h)=y|X(0)=x(0)],\\forall h>0 除此之外则被称为是（时间）非齐次的。齐次马尔可夫过程通常比非齐次的简单，构成了最重要的一类马尔可夫过程。 马尔科夫链 马尔可夫链（Markov Chain, MC）是概率论和数理统计中具有马尔可夫性质（Markov property）且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process） 定义 马尔可夫链是一组具有马尔可夫性质的离散随机变量的集合。具体地，对概率空间 内以一维可数集为指数集（index set） 的随机变量集合，若随机变量的取值都在可数集内： ，且随机变量的条件概率满足如下关系 [2] ： 则被称为马尔可夫链，可数集 被称为状态空间（state space），马尔可夫链在状态空间内的取值称为状态 [2] 。这里定义的马尔可夫链是离散时间马尔可夫链（Discrete-Time MC, DTMC），其具有连续指数集的情形虽然被称为连续时间马尔可夫链（Continuous-Time MC, CTMC），但在本质上是马尔可夫过程（Markov process） [19] 。常见地，马尔可夫链的指数集被称为“步”或“时间步（time-step）。 现实现象解释 现象一：公益组织不能从根本解决贫困人口问题 贫困地区有很多人因为疾病，不良嗜好或其他各种原因不得不去借钱，还不上就变成了贫困户。公益组织即使替他们还清所有债务，甚至每月发补助也改变不了他们的命运。因为公益组织改变的只是他们的初始条件，并没有改变他们下一次得病或者欠债的概率。根据马尔可夫模型，只要概率不变，无伦过程中做任何干预，最终结果不变。 现象二：买彩票中大奖并不会改变人生命运 根据调查，买彩票中大奖的人之后的人生并不是一帆风顺，生活状态反而不如从前。解释有很多种，按照马尔可夫模型的解释是，买彩票中奖只是相当于在某人漫长的人生道路上给了一次干预，而并没有增加他们获得财富能力的概率，因此，即使买彩票中奖也不会走上人生巅峰。同样道理，即使人生中遭遇几次危机甚至破产，只要创造财富能力的概率没变，东山再起也只是时间问题，比如史玉柱。 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-04 09:53:36 "},"CV/Function-Note.html":{"url":"CV/Function-Note.html","title":"函数笔记","keywords":"","body":"函数笔记 transforms.ToTensor() 将numpy的ndarray或PIL.Image读的图片转换成形状为(C,H, W)的Tensor格式，且/255归一化到[0,1.0]之间 math.floor()向下取整 filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。 该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判断， 然后返回 True 或 False，最后将返回 True 的元素放到新列表中 zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。 如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表 Math.ceil() “向上取整”， 即小数部分直接舍去，并向正数部分进1 resize(size, interpolation=2) pytorch函数，图像与处理有两个参数一个是大小H*W,interpolation插值有四种Image.BICUBIC，PIL.Image.LANCZOS，PIL.Image.BILINEAR，PIL.Image.NEAREST torch.cat()是将两个张量（tensor）拼接在一起 unsqueeze()添加维度，squeeze()删除维度 numpy.prod(a, axis=None, dtype=None, out=None, keepdims=, initial=) axis是指求积的维度 keepdims是指保持维度，不缩减 initial是起始数，即返回的矩阵会在元素乘积上再乘起始数 torch.nn.init.normal_(tensor, mean=0, std=1) 初始化服从~N(mean, std) torch.nn.LocalResponseNorm(size: int, alpha: float = 0.0001, beta: float = 0.75, k: float = 1.0) 对由多个输入平面组成的输入信号进行局部响应归一化，其中通道占据第二维。适用于跨通道的归一化 torch.nn.ZeroPad2d(padding: Union[T, Tuple[T, T, T, T]])将输入的张量边界填充为零。对于N维填充，使用torch.nn.functional.pad()。 torch.nn.init.kaimingnormal(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')此为均匀分布，mode- 可选为fan_in 或 fan_out, fan_in使正向传播时，方差一致; fan_out使反向传播时，方差一致 。nonlinearity- 可选 relu 和 leaky_relu ，默认值为 leaky_relu isinstance(object, classinfo) 判断两个类型是否相同，考虑继承关系 torch.nn.init.constant_(tensor, val) 用值val填充向量 Dataloader() dataset(Dataset): 传入的数据集 batch_size(int, optional): 每个batch有多少个样本 shuffle(bool, optional): 在每个epoch开始的时候，对数据进行重新排序 sampler(Sampler, optional): 自定义从数据集中取样本的策略，如果指定这个参数，那么shuffle必须为False batch_sampler(Sampler, optional): 与sampler类似，但是一次只返回一个batch的indices（索引），需要注意的是，一旦指定了这个参数，那么 batch_size,shuffle,sampler,drop_last就不能再制定了（互斥——Mutually exclusive） num_workers (int, optional): 这个参数决定了有几个进程来处理data loading。0意味着所有的数据都会被load进主进程。（默认为0） collate_fn (callable, optional): 将一个list的sample组成一个mini-batch的函数 pin_memory (bool, optional)： 如果设置为True，那么data loader将会在返回它们之前，将tensors拷贝到CUDA中的固定内存（CUDA pinned memory）中. drop_last (bool, optional): 如果设置为True：这个是对最后的未完成的batch来说的，比如你的batch_size设置为64，而一个epoch只有100个样本，那么训练的时候后面的36个就被扔掉了。如果为False（默认），那么会继续正常执行，只是最后的batch_size会小一点 optimizer.zero_grad()把梯度置零 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-20 09:07:08 "},"CV/FPN.html":{"url":"CV/FPN.html","title":"FPN","keywords":"","body":"FPN FPN基本结构 FPN会使用CNN网络中每一层的信息来生成最后的表达特征组合。下图是它的基本架构。从中我们能看到FPN会模型每个CNN层的特征输出进行处理以生成反映此维度信息的特征。而自上至下处理后所生成出的特征之间也有个关联关系，即上层high level的特征会影响下一层次的low level特征表达。最终所有的特征一起用来作为下一步的目标检测或类别分析等任务的输入。 FPN详细介绍 FPN是传统CNN网络对图片信息进行表达输出的一种增强。它目的是为了改进CNN网络的特征提取方式，从而可以使最终输出的特征更好地表示出输入图片各个维度的信息。它的基本过程有三个分别为：自下至上的通路即自下至上的不同维度特征生成；自上至下的通路即自上至下的特征补充增强；CNN网络层特征与最终输出的各维度特征之间的关联表达。 自下至上的通路（Bottom-top pathway）：是指的普通CNN特征自底至上逐层浓缩表达特征的一个过程。此过程很早即被认识到了即较底的层反映较浅层次的图片信息特征像边缘等；较高的层则反映较深层次的图片特征像物体轮廓、乃至类别等； 自上至下的通路（Top-bottome pathway）：上层的特征输出一般其feature map size比较小，但却能表示更大维度（同时也是更加high level）的图片信息。此类high level信息经实验证明能够对后续的目标检测、物体分类等任务发挥关键作用。因此我们在处理每一层信息时会参考上一层的high level信息做为其输入（这里只是在将上层feature map等比例放大后再与本层的feature maps做element wise相加）; CNN层特征与每一级别输出之间的表达关联：在这里作者实验表明使用1x1的Conv即可生成较好的输出特征，它可有效地降低中间层次的channels 数目。最终这些1x1的Convs使得我们输出不同维度的各个feature maps有着相同的channels数目（本文用到的Resnet-101主干网络中，各个层次特征的最终输出channels数目为256） Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 15:53:41 "},"CV/Math.html":{"url":"CV/Math.html","title":"数学知识","keywords":"","body":"数学知识 调和平均数 调和平均数（harmonic mean）又称倒数平均数，是总体各统计变量倒数的算术平均数的倒数。调和平均数有简单调和平均数和加权调和平均数两种。 简单调和平均数 H_n= \\frac{1}{\\frac{1}{n}\\sum\\limits_{i=1}^n\\frac{1}{x_i}}=\\frac{n}{\\sum\\limits_{i=1}^n\\frac{1}{x_i}} 散度 散度（divergence）可用于表征空间各点矢量场发散的强弱程度 KL散度(Kullback-Leibler Divergence) KL散度，也叫相对熵。它衡量的是相同事件空间里的两个概率分布的差异情况，即衡量两个概率分布的相似性的一个度量指标。 信息熵定义如下： H=-\\sum_{i=1}^Np(x_i)logp(x_i) p(xi)表示事件xi发生的概率，信息熵反映的是表示一个概率分布需要的平均信息量 KL散度定义为： D_{KL}(p||q)=\\sum_{i=1}^Np(x_i)\\cdot(log(p(x_i))-log(q(x_i)) 或者 D_{KL}=\\sum_{i=1}^Np(x_i)\\cdot log\\frac{p(x_i)}{q(x_i)} 散度越小说明概率p和q之间越接近，那么估计的概率分布于真实的概率分布也就越接近 KL 散度可以帮助我们选择最优的参数，比如 p(x) 是我们需要估计的一个未知的分布，我们无法直接得知 p(x)的分布，不过我们可以建立一个分布 q(x∣θ)去估计 p(x)，为了确定参数 θ，虽然我们无法得知 p(x)的真实分布，但可以利用采样的方法，从 p(x)中采样 N个样本，构建如下的目标函数： D_{KL}=\\sum_{i=1}^N{\\{log(p(x_i))-log(q(x_i|\\theta))\\}} 因为我们要预估的是参数 θ，上面的第一项 log ⁡p(xi) 与参数 θ无关，所以我们要优化的其实是 −log ⁡q(xi∣θ)，而这个就是我们熟悉的最大似然估计 最大似然估计 利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值 Frobenius norm 对应元素的平方和再开方 ||A||_F=\\sqrt{\\sum_{i-1}^m\\sum_{j=1}^n|a_{ij}|^2} 汉明距离 汉明距离是使用在数据传输差错控制编码里面的，汉明距离是一个概念，它表示两个（相同长度）字对应位不同的数量，我们以d（x,y）表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。 Copyright © ASentry 2020 all right reserved，powered by Gitbook该文章修订时间： 2020-11-16 10:18:29 "}}